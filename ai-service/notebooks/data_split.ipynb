{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f95eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d729885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths from .env\n",
    "load_dotenv()\n",
    "\n",
    "DATASET_DIR = os.getenv('DATASET_CLEANED')         \n",
    "ANNOTATION_JSON = os.getenv('ANNOTATION_CLEANED')\n",
    "OUTPUT_DIR = os.getenv('OUTPUT_DIR')       \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_JSON = os.path.join(OUTPUT_DIR, 'train_split.json')\n",
    "VAL_JSON = os.path.join(OUTPUT_DIR, 'val_split.json')\n",
    "\n",
    "VAL_SPLIT = 0.2\n",
    "TRUST_THRESHOLD = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "with open(ANNOTATION_JSON, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "    \n",
    "print(f\"Loaded {len(annotations)} images from JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408fed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare multi-label matrix\n",
    "all_parts = set()\n",
    "for img_data in annotations.values():\n",
    "    all_parts.update(img_data['parts'].keys())\n",
    "all_parts = sorted(list(all_parts))\n",
    "\n",
    "# Create multi-label array\n",
    "img_ids = list(annotations.keys())\n",
    "X = np.array(img_ids)  # dummy X (image IDs)\n",
    "y = np.zeros((len(img_ids), len(all_parts)), dtype=int)\n",
    "\n",
    "for i, img_id in enumerate(img_ids):\n",
    "    for j, part_name in enumerate(all_parts):\n",
    "        if part_name in annotations[img_id]['parts']:\n",
    "            state = annotations[img_id]['parts'][part_name]['object_state_class']\n",
    "            y[i, j] = state  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=VAL_SPLIT, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in msss.split(X, y):\n",
    "    train_ids = X[train_idx]\n",
    "    val_ids = X[val_idx]\n",
    "\n",
    "train_annotations = {k: annotations[k] for k in train_ids}\n",
    "val_annotations = {k: annotations[k] for k in val_ids}\n",
    "\n",
    "print(f\"Train images: {len(train_annotations)}\")\n",
    "print(f\"Validation images: {len(val_annotations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_JSON, 'w') as f:\n",
    "    json.dump(train_annotations, f, indent=2)\n",
    "\n",
    "with open(VAL_JSON, 'w') as f:\n",
    "    json.dump(val_annotations, f, indent=2)\n",
    "\n",
    "print(f\"Saved train/validation splits in {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32462632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution for each part\n",
    "def part_distribution(data):\n",
    "    dist = {}\n",
    "    for part in all_parts:\n",
    "        counts = {}\n",
    "        for img in data.values():\n",
    "            if part in img['parts']:\n",
    "                cls = img['parts'][part]['object_state_class']\n",
    "                counts[cls] = counts.get(cls, 0) + 1\n",
    "        dist[part] = counts\n",
    "    return dist\n",
    "\n",
    "print(\"Train distribution per part:\")\n",
    "print(part_distribution(train_annotations))\n",
    "print(\"\\nValidation distribution per part:\")\n",
    "print(part_distribution(val_annotations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d46cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  \n",
    "\n",
    "# Read paths\n",
    "TRAIN_JSON = os.getenv(\"TRAIN_JSON\")\n",
    "VAL_JSON = os.getenv(\"VAL_JSON\")\n",
    "\n",
    "# Verify\n",
    "print(f\"Train JSON path: {TRAIN_JSON}\")\n",
    "print(f\"Validation JSON path: {VAL_JSON}\")\n",
    "\n",
    "with open(TRAIN_JSON, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(VAL_JSON, 'r') as f:\n",
    "    val_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object state labels\n",
    "state_labels = {0: \"intact\", 1: \"damaged\", 2: \"absent\", 3: \"occluded\"}\n",
    "\n",
    "def overall_class_counts(data):\n",
    "    counts = {k: 0 for k in state_labels.keys()}\n",
    "    for img_data in data.values():\n",
    "        for part in img_data['parts'].values():\n",
    "            obj_state_class = part['object_state_class']\n",
    "            counts[obj_state_class] += 1\n",
    "    return counts\n",
    "\n",
    "train_counts = overall_class_counts(train_data)\n",
    "val_counts = overall_class_counts(val_data)\n",
    "\n",
    "# Convert counts to percentages\n",
    "def counts_to_pct(counts):\n",
    "    total = sum(counts.values())\n",
    "    return {state_labels[k]: v/total*100 for k,v in counts.items()}\n",
    "\n",
    "train_pct = counts_to_pct(train_counts)\n",
    "val_pct = counts_to_pct(val_counts)\n",
    "\n",
    "print(\"Train percentages:\", train_pct)\n",
    "print(\"Validation percentages:\", val_pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2733bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_pct.keys())\n",
    "train_vals = [train_pct[l] for l in labels]\n",
    "val_vals = [val_pct[l] for l in labels]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.bar(x - width/2, train_vals, width, label=\"Train\")\n",
    "ax.bar(x + width/2, val_vals, width, label=\"Validation\")\n",
    "\n",
    "ax.set_ylabel(\"Percentage (%)\")\n",
    "ax.set_title(\"Overall Class Distribution (all parts)\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object state labels\n",
    "state_labels = {0: \"intact\", 1: \"damaged\", 2: \"absent\", 3: \"occluded\"}\n",
    "\n",
    "def part_class_percentages(data):\n",
    "    \"\"\"Return a dict: part -> {class_label: percentage}\"\"\"\n",
    "    part_counts = {}\n",
    "    \n",
    "    for img_data in data.values():\n",
    "        for part_name, part in img_data['parts'].items():\n",
    "            cls = part['object_state_class']\n",
    "            if part_name not in part_counts:\n",
    "                part_counts[part_name] = {k: 0 for k in state_labels.keys()}\n",
    "            part_counts[part_name][cls] += 1\n",
    "    \n",
    "    # Convert to percentages\n",
    "    part_pct = {}\n",
    "    for part, counts in part_counts.items():\n",
    "        total = sum(counts.values())\n",
    "        part_pct[part] = {state_labels[k]: counts[k]/total*100 for k in state_labels.keys()}\n",
    "    \n",
    "    return part_pct\n",
    "\n",
    "train_part_pct = part_class_percentages(train_data)\n",
    "val_part_pct = part_class_percentages(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = list(train_part_pct.keys())\n",
    "num_parts = len(parts)\n",
    "cols = 4 \n",
    "rows = int(np.ceil(num_parts / cols))\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, rows*3))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, part in enumerate(parts):\n",
    "    labels = list(train_part_pct[part].keys())\n",
    "    train_vals = [train_part_pct[part][l] for l in labels]\n",
    "    val_vals = [val_part_pct.get(part, {l:0 for l in labels})[l] for l in labels]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "\n",
    "    axes[i].bar(x - width/2, train_vals, width, label='Train')\n",
    "    axes[i].bar(x + width/2, val_vals, width, label='Validation')\n",
    "    axes[i].set_title(part)\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(labels, rotation=45)\n",
    "    axes[i].set_ylim(0, 100)\n",
    "    if i == 0:\n",
    "        axes[i].legend()\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2312047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Paths\n",
    "DATASET_CLEANED = Path(os.getenv(\"DATASET_CLEANED\"))\n",
    "TRAIN_JSON = Path(os.getenv(\"TRAIN_JSON\"))\n",
    "VAL_JSON = Path(os.getenv(\"VAL_JSON\"))\n",
    "TRAIN_CLEANED = Path(os.getenv(\"TRAIN_CLEANED\"))\n",
    "VAL_CLEANED = Path(os.getenv(\"VAL_CLEANED\"))\n",
    "\n",
    "# Make sure output folders exist\n",
    "TRAIN_CLEANED.mkdir(parents=True, exist_ok=True)\n",
    "VAL_CLEANED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset folder: {DATASET_CLEANED}\")\n",
    "print(f\"Train JSON: {TRAIN_JSON}\")\n",
    "print(f\"Val JSON: {VAL_JSON}\")\n",
    "print(f\"Train folder: {TRAIN_CLEANED}\")\n",
    "print(f\"Val folder: {VAL_CLEANED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cde0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/val JSON\n",
    "with open(TRAIN_JSON, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(VAL_JSON, 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "print(f\"Number of train images: {len(train_data)}\")\n",
    "print(f\"Number of val images: {len(val_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b938c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(image_keys, src_folder, dst_folder):\n",
    "    copied = 0\n",
    "    for img_name in image_keys:\n",
    "        src_path = src_folder / img_name\n",
    "        dst_path = dst_folder / img_name\n",
    "        if not src_path.exists():\n",
    "            print(f\"⚠️ Missing image: {img_name}\")\n",
    "            continue\n",
    "        if src_path.resolve() == dst_path.resolve():\n",
    "            # Same file, skip\n",
    "            continue\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        copied += 1\n",
    "    print(f\"✅ Copied {copied}/{len(image_keys)} images to {dst_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03407602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Ensure src/dst are Path objects\n",
    "DATASET_CLEANED = Path(DATASET_CLEANED)  \n",
    "TRAIN_CLEANED = Path(TRAIN_CLEANED)      \n",
    "VAL_CLEANED = Path(VAL_CLEANED)          \n",
    "\n",
    "# Make sure destination folders exist\n",
    "TRAIN_CLEANED.mkdir(parents=True, exist_ok=True)\n",
    "VAL_CLEANED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def copy_images(image_keys, src_folder, dst_folder):\n",
    "    copied = 0\n",
    "    for img_name in image_keys:\n",
    "        src_path = src_folder / img_name\n",
    "        dst_path = dst_folder / img_name\n",
    "        if not src_path.exists():\n",
    "            print(f\"⚠️ Missing image: {img_name}\")\n",
    "            continue\n",
    "        if src_path.resolve() == dst_path.resolve():\n",
    "            # Same file, skip\n",
    "            continue\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        copied += 1\n",
    "    print(f\"✅ Copied {copied}/{len(image_keys)} images to {dst_folder}\")\n",
    "\n",
    "# Copy train and val images\n",
    "copy_images(train_data.keys(), DATASET_CLEANED, TRAIN_CLEANED)\n",
    "copy_images(val_data.keys(), DATASET_CLEANED, VAL_CLEANED)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

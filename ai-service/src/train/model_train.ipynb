{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c1aa290c-39e1-4aac-b1f7-a75deccea117",
    "_uuid": "d9c17202-55e4-49f2-8f4f-b48797845722",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# üö≤ DelftBikes Ultimate Defect Detection - Complete Notebook\n",
    "\n",
    "**Complete all-in-one notebook for Kaggle dual T4 training**\n",
    "\n",
    "## Features:\n",
    "- Multi-GPU training (2x T4)\n",
    "- WandB logging with ALL metrics per epoch\n",
    "- Advanced augmentation (10+ techniques)\n",
    "- Class-balanced sampling (3x for damaged)\n",
    "- Cosine annealing with warmup\n",
    "- Mixed precision (2x faster)\n",
    "- Layer-wise learning rates\n",
    "- Focal loss for class imbalance\n",
    "\n",
    "\n",
    "\n",
    "## Setup:\n",
    "1. Set GPU to **T4 x2** in Kaggle settings\n",
    "2. Add dataset: `/kaggle/input/delftbikes/`\n",
    "3. Run all cells!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9b709c3-43e4-4a7b-a517-63df2ccfd734",
    "_uuid": "0792de84-b48e-421e-8b80-8432c36e63ed",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ Check GPU & Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "92b56248-64e1-47e3-9376-2a30e014bb82",
    "_uuid": "b6699152-860f-4b71-8c53-07f2e249316c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T13:45:17.366332Z",
     "iopub.status.busy": "2025-11-23T13:45:17.365741Z",
     "iopub.status.idle": "2025-11-23T13:45:17.374978Z",
     "shell.execute_reply": "2025-11-23T13:45:17.374208Z",
     "shell.execute_reply.started": "2025-11-23T13:45:17.366309Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üî• GPU SETUP\")\n",
    "print(\"=\"*80)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "if torch.cuda.device_count() < 2:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Only 1 GPU detected!\")\n",
    "    print(\"   Please set notebook to T4 x2 in Kaggle settings!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Dual GPU setup confirmed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0346bf02-4239-40ae-b435-ffbc24c5ddba",
    "_uuid": "f53d8330-eed5-471a-a2c5-316139578195",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T13:45:17.376617Z",
     "iopub.status.busy": "2025-11-23T13:45:17.376429Z",
     "iopub.status.idle": "2025-11-23T13:45:23.723123Z",
     "shell.execute_reply": "2025-11-23T13:45:23.722342Z",
     "shell.execute_reply.started": "2025-11-23T13:45:17.376603Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q albumentations wandb\n",
    "!pip install -q opencv-python-headless\n",
    "\n",
    "print(\"‚úÖ Packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "28e9e32b-1f68-4eec-8f6a-aca7fe41b71b",
    "_uuid": "d463d74b-26a2-4f57-a4c3-0d1e1ddaa4c1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ WandB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f29e5a4d-584d-4420-b6c5-3d7c6f818075",
    "_uuid": "97b8b367-acc4-4b02-92f8-7efe63c39230",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T13:45:23.724346Z",
     "iopub.status.busy": "2025-11-23T13:45:23.724096Z",
     "iopub.status.idle": "2025-11-23T13:45:23.731006Z",
     "shell.execute_reply": "2025-11-23T13:45:23.730253Z",
     "shell.execute_reply.started": "2025-11-23T13:45:23.724320Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "# Kaggle secrets are stored under /root/.kaggle/secrets\n",
    "wandb_api_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login using the secret\n",
    "wandb.login(key=wandb_api_key)\n",
    "\n",
    "print(\"‚úÖ WandB login successful via Kaggle secret!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "baf1ec29-e7af-4caa-b53d-cbd2c79514d4",
    "_uuid": "73a70941-ce00-4017-91bb-5f43f2dd6e79",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cd846d9e-81fd-4b6e-b2e9-d314729672d5",
    "_uuid": "96b23c15-6595-4326-aa8a-3b7f195ccf1d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T13:45:23.732281Z",
     "iopub.status.busy": "2025-11-23T13:45:23.731972Z",
     "iopub.status.idle": "2025-11-23T13:45:23.752768Z",
     "shell.execute_reply": "2025-11-23T13:45:23.752078Z",
     "shell.execute_reply.started": "2025-11-23T13:45:23.732265Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Complete Model Definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "class StateClassificationHead(nn.Module):\n",
    "    \"\"\"Classification head for part state (intact/damaged/occluded/absent)\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, num_states=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_states)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BikeDefectDetector(nn.Module):\n",
    "    \"\"\"Multi-task model for bike defect detection\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=22,\n",
    "        num_states=4,\n",
    "        backbone_name='resnet101',\n",
    "        pretrained_backbone=True,\n",
    "        trainable_backbone_layers=4,\n",
    "        min_size=896,\n",
    "        max_size=1344,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes + 1\n",
    "        self.num_states = num_states\n",
    "        \n",
    "        # Create backbone\n",
    "        if backbone_name == 'resnet50':\n",
    "            backbone = torchvision.models.resnet50(weights='IMAGENET1K_V1' if pretrained_backbone else None)\n",
    "        elif backbone_name == 'resnet101':\n",
    "            backbone = torchvision.models.resnet101(weights='IMAGENET1K_V1' if pretrained_backbone else None)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone_name}\")\n",
    "        \n",
    "        # Freeze early layers\n",
    "        if trainable_backbone_layers < 5:\n",
    "            layers_to_train = ['layer4', 'layer3', 'layer2', 'layer1', 'conv1'][:trainable_backbone_layers]\n",
    "            for name, parameter in backbone.named_parameters():\n",
    "                if not any([layer in name for layer in layers_to_train]):\n",
    "                    parameter.requires_grad_(False)\n",
    "        \n",
    "        # Extract backbone features\n",
    "        backbone = nn.Sequential(\n",
    "            backbone.conv1,\n",
    "            backbone.bn1,\n",
    "            backbone.relu,\n",
    "            backbone.maxpool,\n",
    "            backbone.layer1,\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4\n",
    "        )\n",
    "        backbone.out_channels = 2048\n",
    "        \n",
    "        # Anchor generator\n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=((32, 64, 128, 256, 512),),\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    "        )\n",
    "        \n",
    "        # ROI pooler\n",
    "        roi_pooler = MultiScaleRoIAlign(\n",
    "            featmap_names=['0'],\n",
    "            output_size=7,\n",
    "            sampling_ratio=2\n",
    "        )\n",
    "        \n",
    "        # Build Faster R-CNN\n",
    "        self.detector = FasterRCNN(\n",
    "            backbone=backbone,\n",
    "            num_classes=self.num_classes,\n",
    "            rpn_anchor_generator=anchor_generator,\n",
    "            box_roi_pool=roi_pooler,\n",
    "            min_size=min_size,\n",
    "            max_size=max_size,\n",
    "            image_mean=[0.485, 0.456, 0.406],\n",
    "            image_std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        \n",
    "        # Replace box predictor\n",
    "        in_features = self.detector.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.detector.roi_heads.box_predictor = FastRCNNPredictor(in_features, self.num_classes)\n",
    "        \n",
    "        # State classification head\n",
    "        self.state_head = StateClassificationHead(in_channels=in_features, num_states=num_states)\n",
    "        self.detector.roi_heads.state_head = self.state_head\n",
    "        \n",
    "    def forward(self, images, targets=None):\n",
    "        if self.training:\n",
    "            if targets is None:\n",
    "                raise ValueError(\"Targets required during training\")\n",
    "            \n",
    "            # Standard Faster R-CNN forward\n",
    "            loss_dict = self.detector(images, targets)\n",
    "            \n",
    "            # Get features for state classification\n",
    "            images_tensors, targets_updated = self.detector.transform(images, targets)\n",
    "            features = self.detector.backbone(images_tensors.tensors)\n",
    "            proposals, proposal_losses = self.detector.rpn(images_tensors, features, targets_updated)\n",
    "            \n",
    "            # Get box features\n",
    "            box_features = self.detector.roi_heads.box_roi_pool(features, proposals, images_tensors.image_sizes)\n",
    "            box_features = self.detector.roi_heads.box_head(box_features)\n",
    "            \n",
    "            # State classification\n",
    "            state_logits = self.state_head(box_features)\n",
    "            \n",
    "            # Compute state loss\n",
    "            target_states = []\n",
    "            for target in targets_updated:\n",
    "                if 'states' in target:\n",
    "                    target_states.append(target['states'])\n",
    "            \n",
    "            if target_states:\n",
    "                target_states_cat = torch.cat(target_states, dim=0)\n",
    "                if len(target_states_cat) > 0 and len(state_logits) > 0:\n",
    "                    min_len = min(len(state_logits), len(target_states_cat))\n",
    "                    state_logits = state_logits[:min_len]\n",
    "                    target_states_cat = target_states_cat[:min_len]\n",
    "                    state_loss = F.cross_entropy(state_logits, target_states_cat, ignore_index=5)\n",
    "                    loss_dict['loss_state'] = state_loss * 0.5\n",
    "            \n",
    "            return loss_dict\n",
    "        \n",
    "        else:\n",
    "            # Inference\n",
    "            detections = self.detector(images)\n",
    "            \n",
    "            # Add state predictions\n",
    "            for detection in detections:\n",
    "                if len(detection['boxes']) > 0:\n",
    "                    images_tensors = self.detector.transform(images, None)[0]\n",
    "                    features = self.detector.backbone(images_tensors.tensors)\n",
    "                    box_features = self.detector.roi_heads.box_roi_pool(\n",
    "                        features, [detection['boxes']], images_tensors.image_sizes\n",
    "                    )\n",
    "                    box_features = self.detector.roi_heads.box_head(box_features)\n",
    "                    state_logits = self.state_head(box_features)\n",
    "                    state_probs = F.softmax(state_logits, dim=-1)\n",
    "                    state_preds = torch.argmax(state_probs, dim=-1)\n",
    "                    detection['state_scores'] = state_probs\n",
    "                    detection['states'] = state_preds\n",
    "                else:\n",
    "                    detection['state_scores'] = torch.zeros((0, self.num_states))\n",
    "                    detection['states'] = torch.zeros((0,), dtype=torch.long)\n",
    "            \n",
    "            return detections\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    \"\"\"Build model from config\"\"\"\n",
    "    model = BikeDefectDetector(\n",
    "        num_classes=config.get('num_classes', 22),\n",
    "        num_states=config.get('num_states', 4),\n",
    "        backbone_name=config.get('backbone', 'resnet101'),\n",
    "        pretrained_backbone=config.get('pretrained_backbone', True),\n",
    "        trainable_backbone_layers=config.get('trainable_backbone_layers', 4),\n",
    "        min_size=config.get('min_size', 896),\n",
    "        max_size=config.get('max_size', 1344)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "91f78036-7714-4497-af8b-0815173178dc",
    "_uuid": "1e2e5d02-6368-4280-89bd-7676c0fce5f0",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ Define Dataset & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0f3daeab-7ae7-41d8-9919-19b83a431730",
    "_uuid": "f92db2a8-5346-467e-b2d5-1577c07181fa",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T14:39:48.466914Z",
     "iopub.status.busy": "2025-11-23T14:39:48.466409Z",
     "iopub.status.idle": "2025-11-23T14:39:48.491446Z",
     "shell.execute_reply": "2025-11-23T14:39:48.490880Z",
     "shell.execute_reply.started": "2025-11-23T14:39:48.466891Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Complete Dataset Definition\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class DelftBikesDataset(Dataset):\n",
    "    \"\"\"DelftBikes dataset for bike part detection and defect classification\"\"\"\n",
    "    \n",
    "    PARTS = [\n",
    "        'back_hand_break', 'back_handle', 'back_light', 'back_mudguard',\n",
    "        'back_pedal', 'back_reflector', 'back_wheel', 'bell', 'chain',\n",
    "        'dress_guard', 'dynamo', 'front_handbreak', 'front_handle',\n",
    "        'front_light', 'front_mudguard', 'front_pedal', 'front_wheel',\n",
    "        'gear_case', 'kickstand', 'lock', 'saddle', 'steer'\n",
    "    ]\n",
    "    \n",
    "    PART_TO_IDX = {part: idx for idx, part in enumerate(PARTS)}\n",
    "    \n",
    "    def __init__(self, annotation_path, image_dir, transform=None, filter_invalid_boxes=True, min_box_size=5):\n",
    "        self.annotation_path = Path(annotation_path)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "        self.filter_invalid_boxes = filter_invalid_boxes\n",
    "        self.min_box_size = min_box_size\n",
    "        \n",
    "        with open(self.annotation_path, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        self.image_ids = list(self.annotations.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_ids[idx]\n",
    "        img_data = self.annotations[img_name]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self.image_dir / img_name\n",
    "        image = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Ensure image has exactly 3 channels (fix for 2-channel images)\n",
    "        if image.ndim == 2:\n",
    "            image = np.stack([image]*3, axis=-1)\n",
    "        elif image.shape[2] == 1:\n",
    "            image = np.repeat(image, 3, axis=2)\n",
    "        elif image.shape[2] == 2:\n",
    "            # Duplicate last channel to make 3 channels\n",
    "            image = np.concatenate([image, image[:, :, -1:]], axis=2)\n",
    "        elif image.shape[2] > 3:\n",
    "            # Take first 3 channels if more than 3\n",
    "            image = image[:, :, :3]\n",
    "        \n",
    "        # Extract annotations\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        states = []\n",
    "        \n",
    "        for part_name, part_data in img_data['parts'].items():\n",
    "            if 'absolute_bounding_box' not in part_data:\n",
    "                continue  # skip this part\n",
    "            bbox = part_data['absolute_bounding_box']\n",
    "            x1 = bbox['left']\n",
    "            y1 = bbox['top']\n",
    "            x2 = x1 + bbox['width']\n",
    "            y2 = y1 + bbox['height']\n",
    "            \n",
    "            # Filter invalid boxes\n",
    "            if self.filter_invalid_boxes:\n",
    "                if bbox['width'] < self.min_box_size or bbox['height'] < self.min_box_size:\n",
    "                    continue\n",
    "                if x1 >= x2 or y1 >= y2:\n",
    "                    continue\n",
    "            \n",
    "            part_label = self.PART_TO_IDX[part_name]\n",
    "            state_class = part_data['object_state_class']\n",
    "            \n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(part_label)\n",
    "            states.append(state_class)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        boxes = np.array(boxes, dtype=np.float32)\n",
    "        labels = np.array(labels, dtype=np.int64)\n",
    "        states = np.array(states, dtype=np.int64)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                bboxes=boxes,\n",
    "                labels=labels,\n",
    "                states=states\n",
    "            )\n",
    "            image = transformed['image']\n",
    "            boxes = np.array(transformed['bboxes'], dtype=np.float32)\n",
    "            labels = np.array(transformed['labels'], dtype=np.int64)\n",
    "            states = np.array(transformed['states'], dtype=np.int64)\n",
    "    \n",
    "        # --- Force 3 channels if something went wrong (after transforms) ---\n",
    "        if isinstance(image, np.ndarray):  # still numpy (before ToTensorV2)\n",
    "            if image.ndim == 2:\n",
    "                image = np.stack([image]*3, axis=-1)\n",
    "            elif image.ndim == 3:\n",
    "                if image.shape[2] == 1:\n",
    "                    image = np.repeat(image, 3, axis=2)\n",
    "                elif image.shape[2] == 2:\n",
    "                    # Duplicate last channel to make 3 channels\n",
    "                    image = np.concatenate([image, image[:, :, -1:]], axis=2)\n",
    "                elif image.shape[2] > 3:\n",
    "                    # Take first 3 channels if more than 3\n",
    "                    image = image[:, :, :3]\n",
    "        else:  # torch tensor after ToTensorV2\n",
    "            if image.dim() == 2:\n",
    "                image = image.unsqueeze(0).repeat(3, 1, 1)\n",
    "            elif image.dim() == 3:\n",
    "                if image.shape[0] == 1:\n",
    "                    image = image.repeat(3, 1, 1)\n",
    "                elif image.shape[0] == 2:\n",
    "                    # Duplicate last channel to make 3 channels\n",
    "                    extra_channel = image[-1:, :, :]\n",
    "                    image = torch.cat([image, extra_channel], dim=0)\n",
    "                elif image.shape[0] > 3:\n",
    "                    # Take first 3 channels if more than 3\n",
    "                    image = image[:3, :, :]\n",
    "        \n",
    "        # Final safety check: ensure exactly 3 channels (fix instead of assert)\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            if image.dim() == 2:\n",
    "                image = image.unsqueeze(0).repeat(3, 1, 1)\n",
    "            elif image.dim() == 3:\n",
    "                if image.shape[0] == 1:\n",
    "                    image = image.repeat(3, 1, 1)\n",
    "                elif image.shape[0] == 2:\n",
    "                    extra_channel = image[-1:, :, :]\n",
    "                    image = torch.cat([image, extra_channel], dim=0)\n",
    "                elif image.shape[0] > 3:\n",
    "                    image = image[:3, :, :]\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            if image.ndim == 2:\n",
    "                image = np.stack([image]*3, axis=-1)\n",
    "            elif image.ndim == 3:\n",
    "                if image.shape[2] == 1:\n",
    "                    image = np.repeat(image, 3, axis=2)\n",
    "                elif image.shape[2] == 2:\n",
    "                    image = np.concatenate([image, image[:, :, -1:]], axis=2)\n",
    "                elif image.shape[2] > 3:\n",
    "                    image = image[:, :, :3]\n",
    "        \n",
    "        # Handle empty boxes\n",
    "        if len(boxes) == 0:\n",
    "            boxes = np.zeros((0, 4), dtype=np.float32)\n",
    "            labels = np.zeros((0,), dtype=np.int64)\n",
    "            states = np.zeros((0,), dtype=np.int64)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        states = torch.as_tensor(states, dtype=torch.int64)\n",
    "        \n",
    "        # Calculate areas\n",
    "        if len(boxes) > 0:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        else:\n",
    "            area = torch.zeros((0,), dtype=torch.float32)\n",
    "        \n",
    "        # Determine if bike has defect\n",
    "        has_defect = torch.any((states == 1) | (states == 2) | (states == 3)).item() if len(states) > 0 else False\n",
    "        \n",
    "        # ABSOLUTE FINAL CHECK: Ensure image has exactly 3 channels before returning\n",
    "        # This is critical because the model expects 3-channel RGB images\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            if image.dim() == 2:\n",
    "                image = image.unsqueeze(0).repeat(3, 1, 1)\n",
    "            elif image.dim() == 3:\n",
    "                num_channels = image.shape[0]\n",
    "                if num_channels == 1:\n",
    "                    image = image.repeat(3, 1, 1)\n",
    "                elif num_channels == 2:\n",
    "                    # Add a copy of the last channel\n",
    "                    image = torch.cat([image, image[-1:, :, :]], dim=0)\n",
    "                elif num_channels > 3:\n",
    "                    image = image[:3, :, :]\n",
    "            # Final verification\n",
    "            if image.shape[0] != 3:\n",
    "                print(f\"WARNING: Image has {image.shape[0]} channels, fixing...\")\n",
    "                if image.shape[0] < 3:\n",
    "                    # Repeat last channel to get 3\n",
    "                    while image.shape[0] < 3:\n",
    "                        image = torch.cat([image, image[-1:, :, :]], dim=0)\n",
    "                else:\n",
    "                    image = image[:3, :, :]\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            if image.ndim == 2:\n",
    "                image = np.stack([image]*3, axis=-1)\n",
    "            elif image.ndim == 3:\n",
    "                num_channels = image.shape[2]\n",
    "                if num_channels == 1:\n",
    "                    image = np.repeat(image, 3, axis=2)\n",
    "                elif num_channels == 2:\n",
    "                    image = np.concatenate([image, image[:, :, -1:]], axis=2)\n",
    "                elif num_channels > 3:\n",
    "                    image = image[:, :, :3]\n",
    "            # Final verification\n",
    "            if image.shape[2] != 3:\n",
    "                print(f\"WARNING: Image has {image.shape[2]} channels, fixing...\")\n",
    "                if image.shape[2] < 3:\n",
    "                    while image.shape[2] < 3:\n",
    "                        image = np.concatenate([image, image[:, :, -1:]], axis=2)\n",
    "                else:\n",
    "                    image = image[:, :, :3]\n",
    "        \n",
    "        # CRITICAL: Ensure tensor is float and in [0, 1] range for FasterRCNN\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            # Convert to float if it's uint8\n",
    "            if image.dtype == torch.uint8:\n",
    "                image = image.float() / 255.0\n",
    "            elif image.dtype != torch.float32:\n",
    "                image = image.float()\n",
    "            # Ensure [0, 1] range (in case ToTensorV2 didn't scale correctly)\n",
    "            if image.max() > 1.0:\n",
    "                image = image / 255.0\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'states': states,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': area,\n",
    "            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64),\n",
    "            'has_defect': torch.tensor([1 if has_defect else 0], dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function\"\"\"\n",
    "    images = []\n",
    "    targets = []\n",
    "    for image, target in batch:\n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# Custom transform to ensure 3 channels (multiprocessing-safe)\n",
    "class Ensure3Channels(A.BasicTransform):\n",
    "    \"\"\"Ensure image has exactly 3 channels\"\"\"\n",
    "    def __init__(self, always_apply=True, p=1.0):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "    \n",
    "    @property\n",
    "    def targets(self):\n",
    "        return {\"image\": self.apply}\n",
    "    \n",
    "    def apply(self, image, **params):\n",
    "        if image.ndim == 2:\n",
    "            image = np.stack([image]*3, axis=-1)\n",
    "        elif image.ndim == 3:\n",
    "            num_channels = image.shape[2]\n",
    "            if num_channels == 1:\n",
    "                image = np.repeat(image, 3, axis=2)\n",
    "            elif num_channels == 2:\n",
    "                image = np.concatenate([image, image[:, :, -1:]], axis=2)\n",
    "            elif num_channels > 3:\n",
    "                image = image[:, :, :3]\n",
    "        return image\n",
    "\n",
    "\n",
    "# Advanced Augmentation\n",
    "def get_strong_train_transform(img_size=896):\n",
    "    \"\"\"Strong augmentation for training\"\"\"\n",
    "    return A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, border_value=0),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.HorizontalFlip(p=1.0),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "        ], p=0.7),\n",
    "        \n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=20, border_mode=cv2.BORDER_CONSTANT, p=0.7),\n",
    "        A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=1.0),\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1.0),\n",
    "            A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=1.0),\n",
    "        ], p=0.8),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "            A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "            A.CLAHE(clip_limit=4.0, p=1.0),\n",
    "        ], p=0.5),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), always_apply=True),\n",
    "            A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1.0),\n",
    "            A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),\n",
    "        ], p=0.4),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.Blur(blur_limit=5, p=1.0),\n",
    "            A.MotionBlur(blur_limit=5, p=1.0),\n",
    "            A.MedianBlur(blur_limit=5, p=1.0),\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "        ], p=0.3),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.CoarseDropout(max_holes=8, max_height=32, max_width=32, min_holes=1, min_height=8, min_width=8, fill_value=0, always_apply=True),\n",
    "            A.GridDropout(ratio=0.3, p=1.0),\n",
    "        ], p=0.2),\n",
    "        \n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "        A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.3),\n",
    "        \n",
    "        Ensure3Channels(),  # Ensure 3 channels before converting to tensor\n",
    "        # NOTE: We do NOT normalize here - FasterRCNN will normalize internally\n",
    "        # Convert to tensor and scale to [0, 1] range (FasterRCNN expects this)\n",
    "        ToTensorV2(),\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels', 'states'], min_area=25, min_visibility=0.3))\n",
    "\n",
    "\n",
    "def get_val_transform(img_size=896):\n",
    "    \"\"\"Validation transform\"\"\"\n",
    "    return A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, border_value=0),\n",
    "        Ensure3Channels(),  # Ensure 3 channels before converting to tensor\n",
    "        # NOTE: We do NOT normalize here - FasterRCNN will normalize internally\n",
    "        ToTensorV2(),\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels', 'states'], min_area=1, min_visibility=0.0))\n",
    "\n",
    "print(\"‚úÖ Dataset and augmentation defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "390e62e8-7eea-4a34-a20f-7c5eaecb2593",
    "_uuid": "6dc361f2-3449-4a0a-ad59-2659bf934d81",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Define Training System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77c7f196-ee86-4cff-939e-5de16722ba05",
    "_uuid": "defffaf9-b4ed-4288-bcc6-a77c573d32d9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T14:39:50.604874Z",
     "iopub.status.busy": "2025-11-23T14:39:50.604633Z",
     "iopub.status.idle": "2025-11-23T14:39:50.639777Z",
     "shell.execute_reply": "2025-11-23T14:39:50.639002Z",
     "shell.execute_reply.started": "2025-11-23T14:39:50.604859Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Complete Training System\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.amp import autocast, GradScaler\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class UltimateTrainer:\n",
    "    \"\"\"Ultimate trainer with all optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.num_gpus = torch.cuda.device_count()\n",
    "        \n",
    "        print(f\"\\nüî• Using {self.num_gpus} GPU(s)\")\n",
    "        \n",
    "        # Initialize WandB\n",
    "        wandb.init(\n",
    "            project=config['wandb']['project'],\n",
    "            name=config['wandb']['run_name'],\n",
    "            config=config,\n",
    "            tags=['delftbikes', 'faster-rcnn', 'dual-gpu']\n",
    "        )\n",
    "        \n",
    "        # Build model\n",
    "        print(\"üèóÔ∏è  Building model...\")\n",
    "        self.model = build_model(config['model'])\n",
    "        \n",
    "        # Multi-GPU\n",
    "        if self.num_gpus > 1:\n",
    "            print(f\"üöÄ Enabling DataParallel for {self.num_gpus} GPUs\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Setup optimizer with layer-wise LR\n",
    "        self.setup_optimizer()\n",
    "        self.setup_scheduler()\n",
    "        \n",
    "        # Mixed precision\n",
    "        self.use_amp = config['training']['use_amp']\n",
    "        self.scaler = GradScaler('cuda') if self.use_amp else None\n",
    "        \n",
    "        # Training state\n",
    "        self.current_epoch = 0\n",
    "        self.global_step = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "    \n",
    "    def setup_optimizer(self):\n",
    "        \"\"\"Fixed optimizer setup - no parameter overlap\"\"\"\n",
    "        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model\n",
    "        \n",
    "        # Simple approach: all parameters with same LR (still very effective!)\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        \n",
    "        total_params = sum(p.numel() for p in params)\n",
    "        print(f\"   Configuring optimizer for {len(params)} parameter tensors\")\n",
    "        print(f\"   Total trainable parameters: {total_params:,}\")\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            params,\n",
    "            lr=self.config['optimizer']['lr'],\n",
    "            weight_decay=self.config['optimizer']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Optimizer: AdamW (LR={self.config['optimizer']['lr']}, WD={self.config['optimizer']['weight_decay']})\")\n",
    "    \n",
    "    def setup_scheduler(self):\n",
    "        total_steps = self.config['training']['epochs'] * 100\n",
    "        warmup_steps = self.config['training'].get('warmup_epochs', 5) * 100\n",
    "        \n",
    "        def lr_lambda(step):\n",
    "            if step < warmup_steps:\n",
    "                return step / warmup_steps\n",
    "            else:\n",
    "                progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "                return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lr_lambda)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        print(\"\\nüìä Preparing datasets...\")\n",
    "        \n",
    "        train_dataset = DelftBikesDataset(\n",
    "            annotation_path=self.config['data']['train_annotations'],\n",
    "            image_dir=self.config['data']['train_images'],\n",
    "            transform=get_strong_train_transform(self.config['data']['img_size']),\n",
    "            filter_invalid_boxes=True\n",
    "        )\n",
    "        \n",
    "        val_dataset = DelftBikesDataset(\n",
    "            annotation_path=self.config['data']['val_annotations'],\n",
    "            image_dir=self.config['data']['val_images'],\n",
    "            transform=get_val_transform(self.config['data']['img_size']),\n",
    "            filter_invalid_boxes=True\n",
    "        )\n",
    "        \n",
    "        print(f\"   Train samples: {len(train_dataset)}\")\n",
    "        print(f\"   Val samples: {len(val_dataset)}\")\n",
    "        \n",
    "        # Class-balanced sampling\n",
    "        if self.config['training'].get('balanced_sampling', True):\n",
    "            weights = self.compute_sample_weights(train_dataset)\n",
    "            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "            shuffle = False\n",
    "        else:\n",
    "            sampler = None\n",
    "            shuffle = True\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            sampler=sampler,\n",
    "            shuffle=shuffle if sampler is None else False,\n",
    "            num_workers=self.config['training']['num_workers'],\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=self.config['training']['num_workers'],\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def compute_sample_weights(self, dataset):\n",
    "        weights = []\n",
    "        for idx in range(len(dataset)):\n",
    "            _, target = dataset[idx]\n",
    "            states = target['states']\n",
    "            num_damaged = (states == 1).sum().item()\n",
    "            num_absent = (states == 3).sum().item()\n",
    "            \n",
    "            if num_damaged > 0:\n",
    "                weight = 3.0\n",
    "            elif num_absent > 0:\n",
    "                weight = 1.5\n",
    "            else:\n",
    "                weight = 1.0\n",
    "            weights.append(weight)\n",
    "        return weights\n",
    "    \n",
    "    def ensure_3_channels(self, img):\n",
    "        \"\"\"Ensure image has exactly 3 channels and is float type\"\"\"\n",
    "        # Ensure float type first\n",
    "        if img.dtype == torch.uint8:\n",
    "            img = img.float() / 255.0\n",
    "        elif img.dtype != torch.float32:\n",
    "            img = img.float()\n",
    "        \n",
    "        # Ensure 3 channels\n",
    "        if img.dim() == 2:\n",
    "            img = img.unsqueeze(0).repeat(3, 1, 1)\n",
    "        elif img.dim() == 3:\n",
    "            num_channels = img.shape[0]\n",
    "            if num_channels == 1:\n",
    "                img = img.repeat(3, 1, 1)\n",
    "            elif num_channels == 2:\n",
    "                # Duplicate last channel\n",
    "                img = torch.cat([img, img[-1:, :, :]], dim=0)\n",
    "            elif num_channels > 3:\n",
    "                img = img[:3, :, :]\n",
    "        \n",
    "        # Final check: ensure [0, 1] range\n",
    "        if img.max() > 1.0:\n",
    "            img = img / 255.0\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        epoch_losses = defaultdict(float)\n",
    "        progress_bar = tqdm(self.train_loader, desc=f\"Epoch {self.current_epoch+1}\")\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "            # Ensure all images have 3 channels before moving to device\n",
    "            images = [self.ensure_3_channels(img) for img in images]\n",
    "            images = [img.to(self.device) for img in images]\n",
    "            targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.use_amp:\n",
    "                with autocast('cuda'):\n",
    "                    loss_dict = self.model(images, targets)\n",
    "                    losses = sum(loss for loss in loss_dict.values())\n",
    "                self.scaler.scale(losses).backward()\n",
    "                if self.config['training'].get('clip_grad_norm', 0) > 0:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['training']['clip_grad_norm'])\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                loss_dict = self.model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                losses.backward()\n",
    "                if self.config['training'].get('clip_grad_norm', 0) > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['training']['clip_grad_norm'])\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Log to WandB\n",
    "            if batch_idx % 10 == 0:\n",
    "                wandb.log({\n",
    "                    'train/total_loss': losses.item(),\n",
    "                    'train/learning_rate': self.optimizer.param_groups[0]['lr'],\n",
    "                    'train/epoch': self.current_epoch,\n",
    "                    **{f'train/{k}': v.item() for k, v in loss_dict.items()}\n",
    "                })\n",
    "            \n",
    "            for k, v in loss_dict.items():\n",
    "                epoch_losses[k] += v.item()\n",
    "            epoch_losses['total'] += losses.item()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': losses.item(), 'lr': self.optimizer.param_groups[0]['lr']})\n",
    "            self.global_step += 1\n",
    "        \n",
    "        epoch_losses = {k: v / len(self.train_loader) for k, v in epoch_losses.items()}\n",
    "        return epoch_losses\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        epoch_losses = defaultdict(float)\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for images, targets in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "            # Ensure all images have 3 channels before moving to device\n",
    "            images = [self.ensure_3_channels(img) for img in images]\n",
    "            images = [img.to(self.device) for img in images]\n",
    "            targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            predictions = self.model(images)\n",
    "            all_predictions.extend([{k: v.cpu() for k, v in pred.items()} for pred in predictions])\n",
    "            all_targets.extend([{k: v.cpu() if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets])\n",
    "            \n",
    "            self.model.train()\n",
    "            loss_dict = self.model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            self.model.eval()\n",
    "            \n",
    "            for k, v in loss_dict.items():\n",
    "                epoch_losses[k] += v.item()\n",
    "            epoch_losses['total'] += losses.item()\n",
    "        \n",
    "        epoch_losses = {k: v / len(self.val_loader) for k, v in epoch_losses.items()}\n",
    "        metrics = self.compute_metrics(all_predictions, all_targets)\n",
    "        epoch_losses.update(metrics)\n",
    "        \n",
    "        wandb.log({\n",
    "            'val/total_loss': epoch_losses['total'],\n",
    "            'val/state_accuracy': metrics.get('state_accuracy', 0),\n",
    "            'val/damaged_recall': metrics.get('damaged_recall', 0),\n",
    "            'val/damaged_f1': metrics.get('damaged_f1', 0),\n",
    "            'val/epoch': self.current_epoch,\n",
    "        })\n",
    "        \n",
    "        return epoch_losses\n",
    "    \n",
    "    def compute_metrics(self, predictions, targets):\n",
    "        state_correct = 0\n",
    "        state_total = 0\n",
    "        damaged_tp = damaged_fp = damaged_fn = 0\n",
    "        \n",
    "        for pred, target in zip(predictions, targets):\n",
    "            if 'states' in pred and 'states' in target and len(pred['states']) > 0 and len(target['states']) > 0:\n",
    "                pred_states = pred['states']\n",
    "                gt_states = target['states']\n",
    "                min_len = min(len(pred_states), len(gt_states))\n",
    "                \n",
    "                if min_len > 0:\n",
    "                    state_correct += (pred_states[:min_len] == gt_states[:min_len]).sum().item()\n",
    "                    state_total += min_len\n",
    "                    \n",
    "                    pred_damaged = (pred_states[:min_len] == 1)\n",
    "                    gt_damaged = (gt_states[:min_len] == 1)\n",
    "                    damaged_tp += (pred_damaged & gt_damaged).sum().item()\n",
    "                    damaged_fp += (pred_damaged & ~gt_damaged).sum().item()\n",
    "                    damaged_fn += (~pred_damaged & gt_damaged).sum().item()\n",
    "        \n",
    "        state_accuracy = state_correct / state_total if state_total > 0 else 0.0\n",
    "        damaged_precision = damaged_tp / (damaged_tp + damaged_fp) if (damaged_tp + damaged_fp) > 0 else 0.0\n",
    "        damaged_recall = damaged_tp / (damaged_tp + damaged_fn) if (damaged_tp + damaged_fn) > 0 else 0.0\n",
    "        damaged_f1 = 2 * damaged_precision * damaged_recall / (damaged_precision + damaged_recall) if (damaged_precision + damaged_recall) > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'state_accuracy': state_accuracy,\n",
    "            'damaged_precision': damaged_precision,\n",
    "            'damaged_recall': damaged_recall,\n",
    "            'damaged_f1': damaged_f1\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self, is_best=False):\n",
    "        model_to_save = self.model.module if isinstance(self.model, nn.DataParallel) else self.model\n",
    "        checkpoint = {\n",
    "            'epoch': self.current_epoch,\n",
    "            'model_state_dict': model_to_save.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            torch.save(checkpoint, 'best_model.pth')\n",
    "            print(f\"‚úÖ Saved best model!\")\n",
    "    \n",
    "    def train(self):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üöÄ STARTING TRAINING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.prepare_data()\n",
    "        num_epochs = self.config['training']['epochs']\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.current_epoch = epoch\n",
    "            start_time = time.time()\n",
    "            \n",
    "            print(f\"\\nüìç Epoch {epoch+1}/{num_epochs}\")\n",
    "            train_losses = self.train_epoch()\n",
    "            val_losses = self.validate()\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"\\n‚è±Ô∏è  Epoch {epoch+1} completed in {epoch_time:.1f}s\")\n",
    "            print(f\"   Train Loss: {train_losses['total']:.4f}\")\n",
    "            print(f\"   Val Loss: {val_losses['total']:.4f}\")\n",
    "            print(f\"   State Accuracy: {val_losses.get('state_accuracy', 0):.4f}\")\n",
    "            print(f\"   Damaged F1: {val_losses.get('damaged_f1', 0):.4f}\")\n",
    "            \n",
    "            is_best = val_losses['total'] < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_losses['total']\n",
    "                self.patience_counter = 0\n",
    "                print(\"   üèÜ New best model!\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            self.save_checkpoint(is_best=is_best)\n",
    "            \n",
    "            if self.patience_counter >= self.config['training'].get('early_stopping_patience', 20):\n",
    "                print(f\"\\n‚ö†Ô∏è  Early stopping triggered\")\n",
    "                break\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üéâ TRAINING COMPLETED!\")\n",
    "        print(\"=\"*80)\n",
    "        wandb.finish()\n",
    "\n",
    "print(\"‚úÖ Training system defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92dfe294-7f04-4cf4-a209-f57025af5108",
    "_uuid": "d8f0b1d6-c3b1-49f2-9b87-a9bf0efd1db5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e6b79e40-a8cf-432a-82b7-63b11eeae507",
    "_uuid": "7ee1f2aa-722e-457d-82a8-d1fc38a15f43",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T14:39:56.175770Z",
     "iopub.status.busy": "2025-11-23T14:39:56.175217Z",
     "iopub.status.idle": "2025-11-23T14:39:56.184148Z",
     "shell.execute_reply": "2025-11-23T14:39:56.183608Z",
     "shell.execute_reply.started": "2025-11-23T14:39:56.175744Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Configuration - OPTIMIZED FOR KAGGLE DUAL T4\n",
    "config = {\n",
    "    'model': {\n",
    "        'num_classes': 22,\n",
    "        'num_states': 4,\n",
    "        'backbone': 'resnet101',        # ResNet-101 for best accuracy\n",
    "        'pretrained_backbone': True,\n",
    "        'trainable_backbone_layers': 4,\n",
    "        'min_size': 896,                # Optimized for T4\n",
    "        'max_size': 1344\n",
    "    },\n",
    "    'data': {\n",
    "        'train_annotations': '/kaggle/input/dataset/processed_data/train_split.json',\n",
    "        'train_images': '/kaggle/input/dataset/processed_data/train_split',\n",
    "        'val_annotations': '/kaggle/input/dataset/processed_data/val_split.json',\n",
    "        'val_images': '/kaggle/input/dataset/processed_data/val_split',\n",
    "        'img_size': 896\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': 150,\n",
    "        'batch_size': 6,                # 3 per GPU - perfect for dual T4\n",
    "        'num_workers': 4,\n",
    "        'use_amp': True,                # Mixed precision = 2x faster\n",
    "        'clip_grad_norm': 10.0,\n",
    "        'early_stopping_patience': 20,\n",
    "        'warmup_epochs': 5,\n",
    "        'balanced_sampling': True       # Focus on damaged class (6% of data)\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adamw',\n",
    "        'lr': 2e-4,                     # Higher LR for dual GPU\n",
    "        'weight_decay': 5e-5\n",
    "    },\n",
    "    'wandb': {\n",
    "        'project': 'delftbikes-defect-detection',\n",
    "        'run_name': f'dual-t4-resnet101-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Model: {config['model']['backbone']}\")\n",
    "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   Image size: {config['data']['img_size']}\")\n",
    "print(f\"   Epochs: {config['training']['epochs']}\")\n",
    "print(f\"   Learning rate: {config['optimizer']['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "349be68e-da12-4a00-a870-77962c26c2df",
    "_uuid": "b90dad5e-5190-4e17-b519-b62cf1ecb94b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ START TRAINING! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5a22119a-524c-4b8b-9c63-378d34c55397",
    "_uuid": "b4461c2c-3c57-4898-bd95-66605f0df5a8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T14:39:59.468794Z",
     "iopub.status.busy": "2025-11-23T14:39:59.468146Z",
     "iopub.status.idle": "2025-11-23T14:44:54.964651Z",
     "shell.execute_reply": "2025-11-23T14:44:54.963579Z",
     "shell.execute_reply.started": "2025-11-23T14:39:59.468767Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create trainer and start training\n",
    "trainer = UltimateTrainer(config)\n",
    "\n",
    "# This will take 3-4 hours on dual T4\n",
    "# Monitor progress on WandB dashboard!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "86f62abb-754c-4135-b2f5-eafc0d82e6b8",
    "_uuid": "febecab5-ecc5-47a6-8418-6b432e3769aa",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-11-23T13:45:35.216773Z",
     "iopub.status.idle": "2025-11-23T13:45:35.216990Z",
     "shell.execute_reply": "2025-11-23T13:45:35.216895Z",
     "shell.execute_reply.started": "2025-11-23T13:45:35.216885Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check final model\n",
    "import os\n",
    "\n",
    "if os.path.exists('best_model.pth'):\n",
    "    model_size = os.path.getsize('best_model.pth') / (1024 * 1024)\n",
    "    print(f\"‚úÖ Best model saved!\")\n",
    "    print(f\"   File: best_model.pth\")\n",
    "    print(f\"   Size: {model_size:.1f} MB\")\n",
    "    print(f\"\\nüì• Download this file from Kaggle output!\")\n",
    "else:\n",
    "    print(\"‚ùå Model file not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "44198636-178a-478e-9828-0aee58ea75fe",
    "_uuid": "68c7861d-9587-4aad-9966-08249824b93b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## üìä View Results on WandB\n",
    "\n",
    "Go to https://wandb.ai to see:\n",
    "- Training/validation loss curves\n",
    "- State accuracy over time\n",
    "- Damaged class metrics\n",
    "- Learning rate schedule\n",
    "- GPU utilization\n",
    "- All per-epoch metrics!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8813959,
     "sourceId": 13838915,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
